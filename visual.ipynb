{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert video to images\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_to_frames(video_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read the video frame by frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If reading fails (i.e., end of the video), break the loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the current frame as a JPG image\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Saved {frame_count} frames to folder {output_folder}\")\n",
    "\n",
    "seq_name = \"gBR_sBM_c01_d05_mBR0_ch06\"\n",
    "# Example usage\n",
    "video_path = f\"data/aist/videos/{seq_name}.mp4\"\n",
    "output_folder = f\"data/aist/{seq_name}/image\"\n",
    "video_to_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop images\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = f'data/aist/{seq_name}/image/'  # Path to the original image folder\n",
    "output_dir = f'data/aist/{seq_name}/image_crop/'  # Path to the folder where cropped images will be saved\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Define the cropping region\n",
    "y_start = 320   # Retain the part from row 500 to row 1012\n",
    "x_start = 630    # Retain the part from column 650 to column 1162\n",
    "cropped_pixel = 620\n",
    "data = {\n",
    "    \"y_start\": y_start,\n",
    "    \"x_start\": x_start,\n",
    "    \"cropped_pixel\": cropped_pixel\n",
    "}\n",
    "# Write the dictionary to a JSON file\n",
    "with open(os.path.join(output_dir, \"setting.json\"), \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "# Process all images\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):  # Check file format\n",
    "        # Read the image\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Ensure the image was loaded correctly\n",
    "        if image is not None:\n",
    "            # Crop the specified region of the image\n",
    "            cropped_image = image[y_start:y_start+cropped_pixel, x_start:x_start+cropped_pixel]\n",
    "\n",
    "            # Save the cropped image\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(output_path, cropped_image)\n",
    "        else:\n",
    "            print(f\"Failed to load {filename}, skipping...\")\n",
    "\n",
    "print(\"All images have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to video\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def images_to_video(image_folder, output_video, fps=30):\n",
    "    # Get list of images in the folder, sort by filename\n",
    "    images = sorted([img for img in os.listdir(image_folder) if img.endswith(\".jpg\") or img.endswith(\".png\")])\n",
    "\n",
    "    # Check if there are any images in the folder\n",
    "    if not images:\n",
    "        print(\"No images found in the folder.\")\n",
    "        return\n",
    "\n",
    "    # Read the first image to get the size (height, width)\n",
    "    first_image_path = os.path.join(image_folder, images[0])\n",
    "    frame = cv2.imread(first_image_path)\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    # Initialize the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    video = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    # Loop over all the images and write them to the video file\n",
    "    for image in images:\n",
    "        img_path = os.path.join(image_folder, image)\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        # Check if the image was properly read\n",
    "        if frame is None:\n",
    "            print(f\"Skipping {img_path}, unable to read.\")\n",
    "            continue\n",
    "\n",
    "        video.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    video.release()\n",
    "    print(f\"Video saved as {output_video}\")\n",
    "\n",
    "# Example usage\n",
    "# image_folder = f'data/aist/{seq_name}/image_crop'  # Replace with the path to your image folder\n",
    "# output_video = f'data/aist/{seq_name}/image_crop.mp4'       # Desired output video file name\n",
    "image_folder = f'logs/aist_gt_15000step/seq=gBR_sBM_c01_d05_mBR0_ch06_prof=aist_data=aist/viz_only_human_viz/blend_img_opti'  # Replace with the path to your image folder\n",
    "output_video = f'gBR_sBM_c01_d05_mBR0_ch06_opti.mp4'       # Desired output video file name\n",
    "fps = 30  # Frames per second\n",
    "\n",
    "images_to_video(image_folder, output_video, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mask video to mask images\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_to_mask(video_path, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Initialize frame count\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If no frame is returned, video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale (assuming black and white video)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create the mask: set pixels to 255 (white) where the grayscale value is non-zero\n",
    "        mask = cv2.threshold(gray_frame, 100, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        # Save the mask image\n",
    "        mask_filename = os.path.join(output_dir, f\"frame_{frame_count:05d}.png\")\n",
    "        cv2.imwrite(mask_filename, mask)\n",
    "\n",
    "        # Increment the frame count\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(f\"Finished processing {frame_count} frames.\")\n",
    "\n",
    "# Example usage\n",
    "video_path = f'data/aist/{seq_name}/pha.mp4'\n",
    "output_dir = f'data/aist/{seq_name}/mask_crop'\n",
    "video_to_mask(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad mask images\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = f'data/aist/{seq_name}/mask_crop/'  # Path to the folder with cropped masks\n",
    "output_dir = f'data/aist/{seq_name}/mask/'  # Path to the folder where padded masks will be saved\n",
    "\n",
    "# Define the original image size (1920x1080)\n",
    "original_height, original_width = 1080, 1920\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Dimensions and position of the cropped region\n",
    "# y_start, x_start = 320, 630\n",
    "# cropped_pixel = 620  # Retain rows from 500 to 1012\n",
    "\n",
    "# Process all cropped masks\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):  # Check file format\n",
    "        # Read the cropped mask image (in grayscale mode)\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        cropped_mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Ensure it's single-channel\n",
    "\n",
    "        # Ensure the image was loaded correctly\n",
    "        if cropped_mask is not None:\n",
    "            # Create an all-black mask of the original size (1080x1920)\n",
    "            padded_mask = np.zeros((original_height, original_width), dtype=np.uint8)\n",
    "\n",
    "            # Place the cropped mask at the specified position on the all-black image\n",
    "            padded_mask[y_start:y_start+cropped_pixel, x_start:x_start+cropped_pixel] = cropped_mask\n",
    "\n",
    "            # Save the padded mask\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(output_path, padded_mask)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to load {filename}, skipping...\")\n",
    "\n",
    "print(\"All mask images have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform world coordinate to camera\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "from lib_gart.hmr2.models import load_hmr2\n",
    "from pytorch3d.transforms import axis_angle_to_matrix, matrix_to_axis_angle\n",
    "from lib_gart.hmr2.models.smpl_wrapper import SMPL\n",
    "\n",
    "def copy2cpu(tensor):\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return tensor\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generator(points=None, pred_vertices=None, opti_vertices=None, gt_vertices=None, faces=None):\n",
    "    if points is not None:\n",
    "        batch_size = len(points)\n",
    "    elif pred_vertices is not None:\n",
    "        batch_size = len(pred_vertices)\n",
    "    elif opti_vertices is not None:\n",
    "        batch_size = len(opti_vertices)\n",
    "    elif gt_vertices is not None:\n",
    "        batch_size = len(gt_vertices)\n",
    "    for i in range(batch_size):\n",
    "        res = {}\n",
    "        res.update(dict(\n",
    "            points = dict(\n",
    "                pcl = copy2cpu(points[i][:,:3]) if points is not None else None,\n",
    "                # colors = copy2cpu(points[i][:,3:6]) if points[:,3:6] else [0,0,0.8],\n",
    "                color = [0,0,0.8],\n",
    "            ),\n",
    "            pred_verts = dict(\n",
    "                mesh = [copy2cpu(pred_vertices)[i], copy2cpu(faces)] if pred_vertices is not None else None,\n",
    "                color = np.asarray([143, 240, 166]) / 255\n",
    "            ),\n",
    "            opti_verts = dict(\n",
    "                mesh = [copy2cpu(opti_vertices)[i], copy2cpu(faces)] if opti_vertices is not None else None,\n",
    "                color = np.asarray([158, 219, 251]) / 255\n",
    "            ),\n",
    "            label_verts = dict(\n",
    "                mesh = [copy2cpu(gt_vertices)[i], copy2cpu(faces)] if gt_vertices is not None else None,\n",
    "                color = np.asarray([235, 189, 191]) / 255,\n",
    "            ),\n",
    "        ))\n",
    "        yield res\n",
    "\n",
    "seq_name = \"gBR_sBM_c01_d05_mBR0_ch06\"\n",
    "cam = int(seq_name[9:11]) - 1\n",
    "import re\n",
    "seq_label = re.sub(r\"c\\d{2}\", \"cAll\", seq_name)\n",
    "\n",
    "smpl_fn = f\"data/aist/motions/{seq_label}.pkl\"\n",
    "data = np.load(smpl_fn, allow_pickle=True)\n",
    "gt_pose = torch.from_numpy(data['smpl_poses']).reshape(-1, 24, 3).cuda()\n",
    "trans = torch.from_numpy(data['smpl_trans']).cuda()\n",
    "scale = torch.from_numpy(data['smpl_scaling']).cuda()\n",
    "hmr_model, _ = load_hmr2('data/hmr/epoch=35-step=1000000.ckpt')\n",
    "hmr_model = hmr_model.cuda()\n",
    "smpl = SMPL(model_path='data/smpl-meta', num_body_joints=23, mean_params='data/smpl-meta/smpl_mean_params.npz').cuda()\n",
    "with open('data/aist/cameras/setting1.json', 'rt') as f:\n",
    "    setting = json.load(f)\n",
    "K = np.array(setting[cam]['matrix'])\n",
    "R = axis_angle_to_matrix(torch.tensor(setting[cam]['rotation'])[None])[0].cuda()\n",
    "t = torch.tensor(setting[cam]['translation']).cuda()\n",
    "beta_list = []\n",
    "\n",
    "with open(f\"data/aist/keypoints3d/{seq_label}.pkl\", \"rb\") as f:\n",
    "    kp = pickle.load(f)\n",
    "keypoints3d = torch.tensor(kp['keypoints3d']).float().cuda()\n",
    "\n",
    "from lib_render.visualization import StreamVisualization\n",
    "o3d_viz = StreamVisualization()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, img in enumerate(sorted(os.listdir(f'data/aist/{seq_name}/image_crop'))):\n",
    "        rgb = cv2.imread(os.path.join(f'data/aist/{seq_name}/image_crop', img))\n",
    "        rgb = torch.from_numpy(rgb).cuda().permute(2,0,1)[None]/255.\n",
    "        res_rgb = torch.nn.functional.interpolate(rgb, size=(256, 256), mode='bilinear')\n",
    "        hmr_output = hmr_model(res_rgb)\n",
    "        beta_list.append(hmr_output['pred_smpl_params']['betas'])\n",
    "        gt_smpl_params = {}\n",
    "        gt_smpl_params['global_orient'] = axis_angle_to_matrix(gt_pose[i][:1][None])\n",
    "        gt_smpl_params['body_pose'] = axis_angle_to_matrix(gt_pose[i][1:][None])\n",
    "        gt_smpl_params['betas'] = hmr_output['pred_smpl_params']['betas']\n",
    "        gt_smpl_output = smpl(**{k: v.float() for k,v in gt_smpl_params.items()}, pose2rot=False)\n",
    "        root_joint = gt_smpl_output.joints[0, 0]\n",
    "        root_rota = R @ axis_angle_to_matrix(gt_pose[i][:1])\n",
    "        opti_root_pose = matrix_to_axis_angle(root_rota)[0]\n",
    "        opti_trans = R @ trans[i]/scale + t/scale \n",
    "        opti_smpl_params = {}\n",
    "        opti_smpl_params['global_orient'] = axis_angle_to_matrix(opti_root_pose[None])\n",
    "        opti_smpl_params['body_pose'] = axis_angle_to_matrix(gt_pose[i][1:][None])\n",
    "        opti_smpl_params['betas'] = hmr_output['pred_smpl_params']['betas']\n",
    "        opti_smpl_output = smpl(**{k: v.float() for k,v in opti_smpl_params.items()}, pose2rot=False)\n",
    "        offset = (gt_smpl_output.joints[0,0] + trans[i]/scale) @ R.T + t/scale - (opti_smpl_output.joints[0,0] + opti_trans)\n",
    "        gen = generator(\n",
    "            points=(keypoints3d[i]/scale @ R.T + t/scale)[None],\n",
    "            # points=(gt_smpl_output.joints + trans[i]/scale) @ R.T + t/scale,\n",
    "            pred_vertices=hmr_output['pred_vertices'],\n",
    "            opti_vertices=opti_smpl_output.vertices + opti_trans + offset,\n",
    "            gt_vertices=(gt_smpl_output.vertices + trans[i]/scale) @ R.T + t/scale,\n",
    "            faces=smpl.faces\n",
    "        )\n",
    "        o3d_viz.show(gen)\n",
    "        trans[i] = opti_trans + offset\n",
    "        gt_pose[i][:1] = opti_root_pose\n",
    "\n",
    "o3d_viz.close_view()\n",
    "data['smpl_beta'] = torch.cat(beta_list, dim=0).cpu().numpy()\n",
    "data['smpl_poses'] = gt_pose[:-1].cpu().numpy().reshape(-1, 72)\n",
    "data['smpl_trans'] = trans[:-1].cpu().numpy()\n",
    "\n",
    "# with open(f\"data/aist/{seq_name}/smpl.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjun/projects/GART/lib_gart/hmr2/utils/geometry.py:61: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)\n",
      "  b3 = torch.cross(b1, b2)\n",
      "libEGL warning: failed to open /dev/dri/renderD128: Permission denied\n",
      "\n",
      "libEGL warning: NEEDS EXTENSION: falling back to kms_swrast\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'blended_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     89\u001b[0m blended_gt \u001b[38;5;241m=\u001b[39m render_mesh(img, gt_verts, smpl\u001b[38;5;241m.\u001b[39mfaces, K)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(f'test_gt/{img_fname}.jpg', blended_gt)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# pred_verts = copy2cpu(hmr_output[\"pred_vertices\"][0] + trans[i])\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# blended_pred = render_mesh(img, pred_verts, smpl.faces, K)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(f'test_pred/{img_fname}.jpg', blended_pred)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mblended_image\u001b[49m, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     95\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blended_image' is not defined"
     ]
    }
   ],
   "source": [
    "# render mesh to image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pyrender\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib_gart.hmr2.models import load_hmr2\n",
    "from pytorch3d.transforms import axis_angle_to_matrix, matrix_to_axis_angle\n",
    "from lib_gart.hmr2.models.smpl_wrapper import SMPL\n",
    "\n",
    "def render_mesh(img, v, f, K):\n",
    "    mm = trimesh.Trimesh(vertices=v,faces=f)\n",
    "    mesh = pyrender.Mesh.from_trimesh(mm)\n",
    "    scene = pyrender.Scene()\n",
    "    scene.add(mesh)\n",
    "    camera = pyrender.IntrinsicsCamera(fx=K[0,0], fy=K[1,1], cx=K[0,2], cy=K[1,2], znear=0.05, zfar=100000.0, name=None)\n",
    "    camera_pose = np.array([\n",
    "       [1.0,  0.0, 0.0, 0.0],\n",
    "       [0.0,  -1.0, 0.0, 0.0],\n",
    "       [0.0,  0.0, -1.0, 0.0],\n",
    "       [0.0,  0.0, 0.0, 1.0],\n",
    "    ])\n",
    "    scene.add(camera, pose=camera_pose)\n",
    "    light = pyrender.SpotLight(color=np.asarray([158, 219, 251]) / 255, intensity=128,\n",
    "                               innerConeAngle=np.pi/16.0,\n",
    "                               outerConeAngle=np.pi/6.0)\n",
    "    scene.add(light, pose=camera_pose)\n",
    "    r = pyrender.OffscreenRenderer(1920, 1080)\n",
    "    color, depth = r.render(scene)\n",
    "    blend = (color==255)\n",
    "    blend = blend[:,:,0] & blend[:,:,1] & blend[:,:,2]\n",
    "    blend = (1-blend.astype(np.float32)) * 0.8\n",
    "    blend = blend[:,:,None]\n",
    "    blended_image = color*blend + img*(1-blend)\n",
    "    return blended_image\n",
    "\n",
    "\n",
    "def copy2cpu(tensor):\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return tensor\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "seq_name = \"gBR_sBM_c01_d04_mBR0_ch05\"\n",
    "cam = int(seq_name[9:11]) - 1\n",
    "import re\n",
    "seq_label = re.sub(r\"c\\d{2}\", \"cAll\", seq_name)\n",
    "\n",
    "smpl_fn = f\"data/aist/{seq_name}/smpl.pkl\"\n",
    "data = np.load(smpl_fn, allow_pickle=True)\n",
    "gt_pose = torch.from_numpy(data['smpl_poses']).reshape(-1, 24, 3).cuda()\n",
    "trans = torch.from_numpy(data['smpl_trans']).cuda()\n",
    "scale = torch.from_numpy(data['smpl_scaling']).cuda()\n",
    "hmr_model, _ = load_hmr2('data/hmr/epoch=35-step=1000000.ckpt')\n",
    "hmr_model = hmr_model.cuda()\n",
    "smpl = SMPL(model_path='data/smpl-meta', num_body_joints=23, mean_params='data/smpl-meta/smpl_mean_params.npz').cuda()\n",
    "with open('data/aist/cameras/setting1.json', 'rt') as f:\n",
    "    setting = json.load(f)\n",
    "K = np.array(setting[cam]['matrix'])\n",
    "R = axis_angle_to_matrix(torch.tensor(setting[cam]['rotation'])[None])[0].cuda()\n",
    "t = torch.tensor(setting[cam]['translation']).cuda()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for i, img_fname in enumerate(sorted(os.listdir(f'data/aist/{seq_name}/image_crop'))):\n",
    "        # if i != 0:\n",
    "        #     continue\n",
    "        rgb = cv2.imread(os.path.join(f'data/aist/{seq_name}/image_crop', img_fname))\n",
    "        rgb = torch.from_numpy(rgb).cuda().permute(2,0,1)[None]/255.\n",
    "        res_rgb = torch.nn.functional.interpolate(rgb, size=(256, 256), mode='bilinear')\n",
    "        hmr_output = hmr_model(res_rgb)\n",
    "        gt_smpl_params = {}\n",
    "        # rota_pose = R @ axis_angle_to_matrix(gt_pose[i])\n",
    "        # gt_pose[i][0] = matrix_to_axis_angle(rota_pose)[0]\n",
    "        gt_smpl_params['global_orient'] = axis_angle_to_matrix(gt_pose[i][:1][None])\n",
    "        gt_smpl_params['body_pose'] = axis_angle_to_matrix(gt_pose[i][1:][None])\n",
    "        gt_smpl_params['betas'] = hmr_output['pred_smpl_params']['betas']\n",
    "        gt_smpl_output = smpl(**{k: v.float() for k,v in gt_smpl_params.items()}, pose2rot=False)\n",
    "        \n",
    "        gt_verts = copy2cpu(gt_smpl_output.vertices[0] + trans[i])\n",
    "        img = cv2.imread(os.path.join(f'data/aist/{seq_name}/image', img_fname))\n",
    "        blended_gt = render_mesh(img, gt_verts, smpl.faces, K)\n",
    "        cv2.imwrite(f'test_gt/{img_fname}.jpg', blended_gt)\n",
    "        # pred_verts = copy2cpu(hmr_output[\"pred_vertices\"][0] + trans[i])\n",
    "        # blended_pred = render_mesh(img, pred_verts, smpl.faces, K)\n",
    "        # cv2.imwrite(f'test_pred/{img_fname}.jpg', blended_pred)\n",
    "        # plt.imshow(cv2.cvtColor(blended_image, cv2.COLOR_BGR2RGB)/255)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image to gif\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# Define the input directory where images are stored and the output GIF path\n",
    "input_dir = 'test_gt'   # Folder with your images (e.g., 'images/')\n",
    "output_gif = 'test_gt.gif'       # Name for the output GIF\n",
    "\n",
    "# Get a sorted list of image file paths\n",
    "images = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Read images and save them to a GIF\n",
    "with imageio.get_writer(output_gif, mode='I', duration=0.1) as writer:\n",
    "    for filename in images:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "print(\"GIF created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video to gif\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "# Define the input video path and output GIF path\n",
    "input_video = 'test_gt.mp4'  # Replace with your video file\n",
    "output_gif = 'test_gt.gif'\n",
    "\n",
    "# Open the video using OpenCV\n",
    "video = cv2.VideoCapture(input_video)\n",
    "\n",
    "# Get video frame rate and duration per frame for the GIF\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "duration_per_frame = 1 / fps  # Duration per frame in seconds for the GIF\n",
    "\n",
    "# Read frames and write them to the GIF\n",
    "with imageio.get_writer(output_gif, mode='I', duration=duration_per_frame) as writer:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Append the frame to the GIF\n",
    "        writer.append_data(frame_rgb)\n",
    "\n",
    "# Release the video capture\n",
    "video.release()\n",
    "print(\"GIF created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_smpl_params = {}\n",
    "for i in range(100):\n",
    "    # gt_smpl_params['global_orient'] = axis_angle_to_matrix(gt_pose[i][:1][None])\n",
    "    gt_smpl_params['body_pose'] = axis_angle_to_matrix(gt_pose[i][1:][None])\n",
    "    gt_smpl_params['betas'] = beta_list[i]\n",
    "    gt_smpl_output = smpl(**{k: v.float() for k,v in gt_smpl_params.items()}, pose2rot=False)\n",
    "    root_R = axis_angle_to_matrix(gt_pose[i][:1])[0]\n",
    "    j0 = gt_smpl_output.joints[0, 0]\n",
    "    # j0 = root_R @ (j0)\n",
    "    print(j0)\n",
    "    # offset = R @ (j0 - root_R @ j0)\n",
    "    # print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = dict(np.load(\"logs/aist_whmr_15000step/seq=gBR_sBM_c01_d05_mBR0_ch06_prof=aist_data=aist/training_poses.pth\", allow_pickle=True))\n",
    "pose = data[\"training_poses/data.pkl\"]\n",
    "pose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
